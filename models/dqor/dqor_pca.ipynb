{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers as layers\n",
    "import models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse, sys\n",
    "sys.path.append(\"../../\")\n",
    "from utils.generator import ImageAugmentationSequenceH5 # from https://github.com/juselara1/MLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the base xception model\n",
    "\n",
    "input_tensor = tf.keras.layers.Input(shape=(256, 256, 3))\n",
    "base_params = {\"weights\": \"imagenet\", \"input_tensor\": input_tensor, \"include_top\": False, \"pooling\": \"avg\"}\n",
    "\n",
    "preprop = tf.keras.applications.xception.preprocess_input\n",
    "base = tf.keras.applications.xception.Xception(**base_params)\n",
    "train_batch = 32\n",
    "val_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation set up\n",
    "\n",
    "augmentation = {\"preprocessing_function\": preprop, \"apply\":True,\n",
    "                    \"random_brightness\": {\"max_delta\": 0.3},\n",
    "                    \"random_contrast\": {\"lower\":0.7, \"upper\":1},\n",
    "                    \"random_hue\": {\"max_delta\": 0.1},\n",
    "                    \"random_saturation\": {\"lower\": 0.7, \"upper\":1},\n",
    "                    \"random_rotation\": {\"minval\": 0, \"maxval\": 2*np.pi},\n",
    "                    \"horizontal_flip\": True, \"vertical_flip\": True\n",
    "                   }\n",
    "\n",
    "\n",
    "train_seq = ImageAugmentationSequenceH5('/home/stoledoc/work/data1/images.h5', \"ims_train\", \"y_train\", num_classes=5,\n",
    "                                        #labels_transform=lambda i:np.int32(i>1),\n",
    "                                        #labels_transform=lambda i:np.eye(5)[i],\n",
    "                                        class_weighted=False, categorical=False, batch_size=train_batch,\n",
    "                                        #shuffle=True)\n",
    "                                        augmentation = augmentation, shuffle=True)\n",
    "\n",
    "augmentation = {\"preprocessing_function\": preprop, \"apply\":False}\n",
    "\n",
    "val_seq = ImageAugmentationSequenceH5('/home/stoledoc/work/data1/images.h5', \"ims_val\", \"y_val\", num_classes=5,\n",
    "                                      #labels_transform=lambda i:np.int32(i>1),\n",
    "                                      #labels_transform=lambda i:np.eye(5)[i],\n",
    "                                      class_weighted=False, categorical=False, batch_size=val_batch,\n",
    "                                      #shuffle=False)\n",
    "                                      augmentation = augmentation, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Checkpoint for warming up\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath='/home/stoledoc/work/datanfs/stoledoc/MICCAI_2021/xception+qmr_warming_up_2.h5', monitor=\"val_loss\", \n",
    "                                              verbose=True, save_best_only=True,\n",
    "                                              save_weights_only=True, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QMR model layers set up\n",
    "\n",
    "fm_x = layers.QFeatureMapRFF(\n",
    "                            input_dim=2048,\n",
    "                            dim=1024, \n",
    "                            gamma=0.0006740385616590121, \n",
    "                            random_state=1)\n",
    "qm = layers.QMeasureClassifEig(dim_x=1024, dim_y=5, num_eig=32)\n",
    "\n",
    "dmregress = layers.DensityMatrixRegression()\n",
    "\n",
    "dm2dist = layers.DensityMatrix2Dist()\n",
    "\n",
    "#QMR layers\n",
    "\n",
    "psi_x = fm_x(base.output)\n",
    "rho_y = qm(psi_x)\n",
    "output = dmregress(rho_y)\n",
    "output_2 = dm2dist(rho_y)\n",
    "\n",
    "#Final model\n",
    "\n",
    "#Freezing the layers for warming up:\n",
    "for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=[output_2, output])\n",
    "\n",
    "#QMR loss\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred[:,0:1]*4) + 0.5526216196078314 * y_pred[:, 1:2]\n",
    "\n",
    "model.compile(loss=loss, optimizer=tf.optimizers.Adam(lr=0.0001947103376276921))\n",
    "\n",
    "#Loading weights from JSLara\n",
    "\n",
    "model.load_weights('models/xception.h5', by_name = True)\n",
    "\n",
    "model.fit(train_seq, steps_per_epoch=len(train_seq), validation_data=val_seq, validation_steps=len(val_seq),\n",
    "          epochs=20, callbacks=[callback], shuffle=True, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnFreezing the layers after warming up:\n",
    "for layer in base.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model.compile(loss=loss, optimizer=tf.optimizers.Adam(lr=1e-7))\n",
    "model.load_weights('/home/stoledoc/work/datanfs/stoledoc/MICCAI_2021/xception+qmr_warming_up_2.h5', by_name = True)\n",
    "\n",
    "#Model Checkpoint for Full train\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath='/home/stoledoc/work/datanfs/stoledoc/MICCAI_2021/xception+qmr_full_train_3.h5', monitor=\"val_loss\", \n",
    "                                              verbose=True, save_best_only=True,\n",
    "                                              save_weights_only=True, mode=\"min\")\n",
    "\n",
    "model.fit(train_seq, steps_per_epoch=len(train_seq), validation_data=val_seq, validation_steps=len(val_seq),\n",
    "          epochs=20, callbacks=[callback], shuffle=True, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_route = '/home/stoledoc/work/data1/images.h5'\n",
    "\n",
    "with h5py.File(file_route,'r') as h5:\n",
    "\n",
    "    \n",
    "    #X_train = h5['ims_train'].value\n",
    "    #X_val = h5['ims_val'].value\n",
    "    X_test = h5['ims_test'].value\n",
    "    \n",
    "    #y_train = h5['y_train'].value\n",
    "    #y_val = h5['y_val'].value\n",
    "    y_test = h5['y_test'].value\n",
    "    #id_ims_train = h5['id_train'].value\n",
    "    #id_ims_val = h5['id_val'].value\n",
    "    id_ims_test = h5['id_test'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading weights \n",
    "\n",
    "model.load_weights('/home/stoledoc/work/datanfs/stoledoc/MICCAI_2021/xception+qmr_full_train_3.h5')#, by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = {\"preprocessing_function\": preprop, \"apply\":False}\n",
    "\n",
    "test_seq = ImageAugmentationSequenceH5('/home/stoledoc/work/data1/images.h5', \"ims_test\", \"y_test\", num_classes=5,\n",
    "                                       #labels_transform=lambda i:np.int32(i>1), \n",
    "                                       return_y=False,\n",
    "                                       class_weighted=False, categorical=False, batch_size=20,\n",
    "                                       augmentation = augmentation, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2/4387 [..............................] - ETA: 3:55WARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0137s vs `on_predict_batch_end` time: 0.1095s). Check your callbacks.\n",
      "4387/4387 [==============================] - 460s 105ms/step\n"
     ]
    }
   ],
   "source": [
    "out = model.predict(test_seq, steps=len(test_seq), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87740, 5), (87740, 2))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87740, 5), (87740, 2))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta evaluaci√≤n tomamos el modelo que bota solo el mean y el std. Pilas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  roc_curve, roc_auc_score, classification_report, confusion_matrix, accuracy_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, std = out[1][:, 0], np.sqrt(out[1][:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6660456 , 0.46914953, 0.6110195 , ..., 0.49020052, 0.27564344,\n",
       "       0.63827837], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4], dtype=uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------MAE = 0.87410533\n"
     ]
    }
   ],
   "source": [
    "print('------------MAE =', mean_absolute_error(y_test, np.rint(y_pred*4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1527    0.0997    0.1207      7640\n",
      "           1     0.6630    0.4249    0.5179     32302\n",
      "           2     0.2536    0.4742    0.3305     15301\n",
      "           3     0.4304    0.4707    0.4497     30560\n",
      "           4     0.0000    0.0000    0.0000      1937\n",
      "\n",
      "    accuracy                         0.4118     87740\n",
      "   macro avg     0.3000    0.2939    0.2837     87740\n",
      "weighted avg     0.4515    0.4118    0.4154     87740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.rint(y_pred*4), digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Level: Mayority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.rint(y_pred*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(set(id_ims_test))\n",
    "keys.sort()\n",
    "#keys = list(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_0 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_0[i] = 0\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "  if y_pred[i] == 0:\n",
    "    d_0[id_ims_test[i]] += 1\n",
    "\n",
    "d_1 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_1[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_pred[i] == 1:\n",
    "    d_1[id_ims_test[i]] += 1\n",
    "\n",
    "d_2 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_2[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_pred[i] == 2:\n",
    "    d_2[id_ims_test[i]] += 1\n",
    "\n",
    "d_3 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_3[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_pred[i] == 3:\n",
    "    d_3[id_ims_test[i]] += 1\n",
    "\n",
    "d_4 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_4[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_pred[i] == 4:\n",
    "    d_4[id_ims_test[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_pred = []\n",
    "\n",
    "for i in range(len(keys)):\n",
    "  vote = np.argmax([d_0[keys[i]], d_1[keys[i]], d_2[keys[i]], d_3[keys[i]], d_4[keys[i]]])\n",
    "  vote_pred.append(vote)\n",
    "\n",
    "vote_pred = np.asarray(vote_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_0 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_0[i] = 0\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "  if y_test[i] == 0:\n",
    "    d_0[id_ims_test[i]] += 1\n",
    "\n",
    "d_1 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_1[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_test[i] == 1:\n",
    "    d_1[id_ims_test[i]] += 1\n",
    "\n",
    "d_2 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_2[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_test[i] == 2:\n",
    "    d_2[id_ims_test[i]] += 1\n",
    "\n",
    "d_3 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_3[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_test[i] == 3:\n",
    "    d_3[id_ims_test[i]] += 1\n",
    "\n",
    "d_4 = {} \n",
    "  \n",
    "for i in keys: \n",
    "    d_4[i] = 0\n",
    "\n",
    "for i in range(id_ims_test.shape[0]):\n",
    "  if y_test[i] == 4:\n",
    "    d_4[id_ims_test[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_test = []\n",
    "\n",
    "for i in range(len(keys)):\n",
    "  vote = np.argmax([d_0[keys[i]], d_1[keys[i]], d_2[keys[i]], d_3[keys[i]], d_4[keys[i]]])\n",
    "  vote_test.append(vote)\n",
    "\n",
    "vote_test = np.asarray(vote_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1,\n",
       "       4, 2, 3, 3, 3, 1, 2, 2, 0, 2, 0, 2, 2, 2, 3, 3, 2, 1, 3, 3, 0, 3,\n",
       "       3, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------MAE = 0.6304347826086957\n"
     ]
    }
   ],
   "source": [
    "print('------------MAE =', mean_absolute_error(vote_test, vote_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         4\n",
      "           1     0.9231    0.7059    0.8000        17\n",
      "           2     0.2778    0.6250    0.3846         8\n",
      "           3     0.5333    0.5000    0.5161        16\n",
      "           4     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.5435        46\n",
      "   macro avg     0.3468    0.3662    0.3401        46\n",
      "weighted avg     0.5750    0.5435    0.5421        46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(vote_test, vote_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  3  1  0]\n",
      " [ 0 12  3  2  0]\n",
      " [ 0  0  5  3  0]\n",
      " [ 0  1  7  8  0]\n",
      " [ 0  0  0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(vote_test, vote_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Level: Probability Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_pred = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dist_pred = {}\n",
    "for i in keys: \n",
    "    dict_dist_pred[i] = [0, 0, 0, 0, 0] # Initializing a null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_test.shape[0]):\n",
    "    dict_dist_pred[id_ims_test[i]] = dict_dist_pred[id_ims_test[i]] + dist_pred[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just picking the argmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pred = []\n",
    "\n",
    "for i in range(len(keys)):\n",
    "  prob = np.argmax(dict_dist_pred[keys[i]])\n",
    "  prob_pred.append(prob)\n",
    "\n",
    "prob_pred = np.asarray(prob_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3,\n",
       "       3, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------MAE = 0.717391304347826\n"
     ]
    }
   ],
   "source": [
    "print('------------MAE =', mean_absolute_error(vote_test, prob_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         4\n",
      "           1     0.7368    0.8235    0.7778        17\n",
      "           2     0.5000    0.3750    0.4286         8\n",
      "           3     0.4762    0.6250    0.5405        16\n",
      "           4     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.5870        46\n",
      "   macro avg     0.3426    0.3647    0.3494        46\n",
      "weighted avg     0.5249    0.5870    0.5500        46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(vote_test, prob_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  3  0]\n",
      " [ 0 14  1  2  0]\n",
      " [ 0  0  3  5  0]\n",
      " [ 0  5  1 10  0]\n",
      " [ 0  0  0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(vote_test, prob_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now computing the expected value and setting it as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1,2,3,4]\n",
    "E_pred = []\n",
    "for i in range(len(keys)):\n",
    "  dist = dict_dist_pred[keys[i]]/np.sum(dict_dist_pred[keys[i]])\n",
    "  prediction = np.rint(np.dot(dist, labels))\n",
    "  E_pred.append(prediction)\n",
    "E_pred = np.asarray(E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 2., 1., 1., 2., 2., 2., 1., 1., 1., 1., 1., 2., 2., 2., 1.,\n",
       "       1., 2., 2., 1., 1., 2., 2., 3., 3., 2., 2., 2., 3., 2., 2., 3., 2.,\n",
       "       2., 2., 3., 2., 3., 3., 2., 3., 2., 3., 3., 3.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------MAE = 0.6521739130434783\n"
     ]
    }
   ],
   "source": [
    "print('------------MAE =', mean_absolute_error(vote_test, E_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         4\n",
      "           1     1.0000    0.6471    0.7857        17\n",
      "           2     0.2609    0.7500    0.3871         8\n",
      "           3     0.5833    0.4375    0.5000        16\n",
      "           4     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.5217        46\n",
      "   macro avg     0.3688    0.3669    0.3346        46\n",
      "weighted avg     0.6178    0.5217    0.5316        46\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(vote_test, E_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  3  1  0]\n",
      " [ 0 11  4  2  0]\n",
      " [ 0  0  6  2  0]\n",
      " [ 0  0  9  7  0]\n",
      " [ 0  0  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(vote_test, E_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
